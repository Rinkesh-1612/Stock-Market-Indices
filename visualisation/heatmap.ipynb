{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce64888d",
   "metadata": {},
   "source": [
    "Index to ticker mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e5b3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import logging\n",
    "import io\n",
    "\n",
    "# --- Configuration ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "OUTPUT_DIR = \"index_market_data\"\n",
    "\n",
    "# --- Advanced, Configuration-Driven Ticker Scraping Engine ---\n",
    "\n",
    "INDEX_CONFIG = {\n",
    "    \"sp500\": {\n",
    "        \"name\": \"S&P 500\",\n",
    "        \"url\": \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\",\n",
    "        \"table_identifier\": {'id': 'constituents'}, \n",
    "        \"ticker_column\": \"Symbol\",\n",
    "        \"clean_fn\": lambda s: s.replace('.', '-')\n",
    "    },\n",
    "    \"dowjones\": {\n",
    "        \"name\": \"Dow Jones\",\n",
    "        \"url\": \"https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average\",\n",
    "        \"table_identifier\": {'class': 'wikitable'},\n",
    "        \"ticker_column\": \"Symbol\",\n",
    "        \"clean_fn\": lambda s: s\n",
    "    },\n",
    "    \"nasdaq100\": {\n",
    "        \"name\": \"NASDAQ 100\",\n",
    "        \"url\": \"https://en.wikipedia.org/wiki/Nasdaq-100\",\n",
    "        \"table_identifier\": {'id': 'constituents'},\n",
    "        \"ticker_column\": \"Ticker\",\n",
    "        \"clean_fn\": lambda s: s.replace('.', '-')\n",
    "    },\n",
    "    \"nifty50\": {\n",
    "        \"name\": \"Nifty 50\",\n",
    "        \"url\": \"https://en.wikipedia.org/wiki/NIFTY_50\",\n",
    "        \"table_identifier\": {'class': 'wikitable sortable'},\n",
    "        \"ticker_column\": \"Symbol\",\n",
    "        \"clean_fn\": lambda s: f\"{s}.NS\" # Add .NS suffix for Indian stocks\n",
    "    },\n",
    "    \"ftse100\": {\n",
    "        \"name\": \"FTSE 100\",\n",
    "        \"url\": \"https://en.wikipedia.org/wiki/FTSE_100_Index\",\n",
    "        \"table_identifier\": {'id': 'constituents'},\n",
    "        \"ticker_column\": \"EPIC\",\n",
    "        \"clean_fn\": lambda s: f\"{s}.L\" if '.' not in s else s\n",
    "    },\n",
    "}\n",
    "\n",
    "def get_tickers_from_wikipedia(config):\n",
    "    \"\"\"Scrapes tickers from a Wikipedia page based on a flexible configuration.\"\"\"\n",
    "    name = config['name']\n",
    "    url = config['url']\n",
    "    logging.info(f\"Scraping {name} tickers from Wikipedia...\")\n",
    "    \n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        \n",
    "        table = soup.find('table', config['table_identifier'])\n",
    "        if table is None:\n",
    "            logging.error(f\"Could not find the specified table for {name}.\")\n",
    "            return []\n",
    "\n",
    "        df = pd.read_html(io.StringIO(str(table)))[0]\n",
    "        \n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            df.columns = ['_'.join(map(str, col)).strip() for col in df.columns.values]\n",
    "        \n",
    "        ticker_col = config['ticker_column']\n",
    "        if ticker_col not in df.columns:\n",
    "            logging.error(f\"Ticker column '{ticker_col}' not found for {name}.\")\n",
    "            return []\n",
    "            \n",
    "        tickers = df[ticker_col].astype(str).apply(config['clean_fn']).tolist()\n",
    "        logging.info(f\"Found {len(tickers)} tickers for {name}.\")\n",
    "        return tickers\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while scraping {name}: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_russell_2000_tickers():\n",
    "    \"\"\"Scrapes Russell 2000 tickers from a non-Wikipedia source.\"\"\"\n",
    "    logging.info(\"Scraping Russell 2000 tickers...\")\n",
    "    url = 'https://www.lazyfa.com/screener/russell-2000'\n",
    "    try:\n",
    "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        table = soup.find('table')\n",
    "        tickers = []\n",
    "        if table:\n",
    "            for row in table.findAll('tr')[1:]:\n",
    "                try:\n",
    "                    ticker = row.findAll('td')[0].text.strip()\n",
    "                    tickers.append(ticker)\n",
    "                except IndexError:\n",
    "                    continue\n",
    "        logging.info(f\"Found {len(tickers)} Russell 2000 tickers.\")\n",
    "        return tickers\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to scrape Russell 2000 tickers: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    all_ticker_mappings = []\n",
    "    \n",
    "    # Scrape tickers from Wikipedia config\n",
    "    for config in INDEX_CONFIG.values():\n",
    "        tickers = get_tickers_from_wikipedia(config)\n",
    "        # Add a mapping for each ticker to its index\n",
    "        for ticker in tickers:\n",
    "            all_ticker_mappings.append({\n",
    "                \"Ticker\": ticker,\n",
    "                \"Index\": config['name']\n",
    "            })\n",
    "        \n",
    "    # Scrape Russell 2000 tickers\n",
    "    russell_tickers = get_russell_2000_tickers()\n",
    "    for ticker in russell_tickers:\n",
    "        all_ticker_mappings.append({\n",
    "            \"Ticker\": ticker,\n",
    "            \"Index\": \"Russell 2000\"\n",
    "        })\n",
    "\n",
    "    # --- Process and Save the final list ---\n",
    "    df_mappings = pd.DataFrame(all_ticker_mappings)\n",
    "    df_mappings.drop_duplicates(inplace=True)\n",
    "    logging.info(f\"Found a total of {len(df_mappings)} ticker-to-index mappings.\")\n",
    "    \n",
    "    # Save the mapping file (for Script 3)\n",
    "    output_path = os.path.join(OUTPUT_DIR, \"ticker_to_index_map.csv\")\n",
    "    df_mappings.to_csv(output_path, index=False)\n",
    "    logging.info(f\"Successfully saved ticker-to-index mappings to: {output_path}\")\n",
    "\n",
    "    # Save the unique list (for Script 2)\n",
    "    unique_tickers = sorted(list(df_mappings['Ticker'].unique()))\n",
    "    output_path_unique = os.path.join(OUTPUT_DIR, \"all_scraped_tickers.csv\")\n",
    "    ticker_df = pd.DataFrame(unique_tickers, columns=[\"Ticker\"])\n",
    "    ticker_df.to_csv(output_path_unique, index=False)\n",
    "    logging.info(f\"Saved unique ticker list to: {output_path_unique}\")\n",
    "\n",
    "    logging.info(\"Ticker scraping process complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fc9764",
   "metadata": {},
   "source": [
    "Stock values finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f08587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "TICKER_DIR = \"index_market_data\"\n",
    "\n",
    "# --- File Paths ---\n",
    "TICKER_FILE = os.path.join(TICKER_DIR, \"all_scraped_tickers.csv\")\n",
    "PRICE_OUTPUT_FILE = os.path.join(TICKER_DIR, \"ticker_historical_prices.csv\")\n",
    "INFO_OUTPUT_FILE = os.path.join(TICKER_DIR, \"company_info.csv\")\n",
    "\n",
    "# --- Tunables ---\n",
    "SLEEP_TIMER = 0.1 # Small delay for the .info fetch to avoid rate-limiting\n",
    "\n",
    "\n",
    "def get_specific_prices(tickers_list):\n",
    "    \"\"\"\n",
    "    Fetches specific historical closing prices for a list of tickers in a fast batch.\n",
    "    *** MODIFIED to handle live, incomplete data. ***\n",
    "    \"\"\"\n",
    "    logging.info(\"--- Starting Price Data Fetch (Batch Download) ---\")\n",
    "    if not tickers_list:\n",
    "        logging.warning(\"No tickers provided for price fetch.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Download '2mo' of data to ensure we have ~21 trading days\n",
    "        data = yf.download(tickers_list, period=\"2mo\", interval=\"1d\", progress=True)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during yfinance price download: {e}\")\n",
    "        return\n",
    "        \n",
    "    if data.empty:\n",
    "        logging.warning(\"No price data was returned from yfinance.\")\n",
    "        return\n",
    "        \n",
    "    logging.info(\"Price download complete. Processing data...\")\n",
    "    \n",
    "    close_prices = data['Close']\n",
    "    \n",
    "    # --- START OF MODIFICATION ---\n",
    "    # Check if the last row (today's live data) has ANY NaN values.\n",
    "    # If it does, it means at least one market is closed, and the data is incomplete.\n",
    "    # We drop this row to use the last *full* trading day as our T-0.\n",
    "    if not close_prices.empty and close_prices.iloc[-1].isnull().any():\n",
    "        logging.warning(\"Dropping last row due to NaN values (market likely open/data incomplete).\")\n",
    "        close_prices = close_prices.iloc[:-1] # Get all rows *except* the last one\n",
    "    # --- END OF MODIFICATION ---\n",
    "\n",
    "    \n",
    "    # Check if we still have enough data after (potentially) dropping a row\n",
    "    if len(close_prices) < 22:\n",
    "        logging.warning(f\"Not enough data (need ~22 days, got {len(close_prices)}). Price output may be incomplete.\")\n",
    "        df_summary = pd.DataFrame(columns=[\n",
    "            'T-0 (Most Recent)', 'T-1 (Previous Day)', 'T-5 (~1 Week Ago)', 'T-21 (~1 Month Ago)'\n",
    "        ])\n",
    "    else:\n",
    "        # These indices now correctly reference the last *full* day's close\n",
    "        df_summary = pd.DataFrame({\n",
    "            'T-0 (Most Recent)': close_prices.iloc[-1],\n",
    "            'T-1 (Previous Day)': close_prices.iloc[-2],\n",
    "            'T-5 (~1 Week Ago)': close_prices.iloc[-6],\n",
    "            'T-21 (~1 Month Ago)': close_prices.iloc[-22]\n",
    "        })\n",
    "    \n",
    "    df_summary = df_summary.dropna(how='all').round(2)\n",
    "    df_summary.index.name = \"Ticker\"\n",
    "    \n",
    "    try:\n",
    "        df_summary.to_csv(PRICE_OUTPUT_FILE)\n",
    "        logging.info(f\"--- Successfully saved price data to: {PRICE_OUTPUT_FILE} ---\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save price data. Error: {e}\")\n",
    "\n",
    "def fetch_company_info(tickers_list):\n",
    "    \"\"\"\n",
    "    Fetches sector and market cap for all tickers.\n",
    "    This is a SLOW process as it requires individual API calls.\n",
    "    \"\"\"\n",
    "    logging.info(\"--- Starting Company Info Fetch (Sector/MarketCap) ---\")\n",
    "    if not tickers_list:\n",
    "        logging.warning(\"No tickers provided for company info fetch.\")\n",
    "        return\n",
    "\n",
    "    all_info_data = []\n",
    "    total_tickers = len(tickers_list)\n",
    "    logging.warning(f\"This will take a long time (Approx { (total_tickers * SLEEP_TIMER) / 60 :.1f} minutes).\")\n",
    "\n",
    "    for i, ticker_symbol in enumerate(tickers_list):\n",
    "        if (i+1) % 50 == 0:\n",
    "            logging.info(f\"Progress: {i+1} / {total_tickers} tickers processed.\")\n",
    "            \n",
    "        try:\n",
    "            ticker = yf.Ticker(ticker_symbol)\n",
    "            info = ticker.info\n",
    "\n",
    "            data = {\n",
    "                \"Ticker\": ticker_symbol,\n",
    "                \"Sector\": info.get('sector', 'N/A'),\n",
    "                \"Industry\": info.get('industry', 'N_A'),\n",
    "                \"MarketCap\": info.get('marketCap', 0)\n",
    "            }\n",
    "            all_info_data.append(data)\n",
    "            time.sleep(SLEEP_TIMER)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Could not get .info for {ticker_symbol}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "    logging.info(\"--- Info fetch complete, saving data ---\")\n",
    "    \n",
    "    if not all_info_data:\n",
    "        logging.error(\"No company info was fetched.\")\n",
    "        return\n",
    "\n",
    "    df_info = pd.DataFrame(all_info_data)\n",
    "    df_info = df_info[df_info['MarketCap'] > 0] # Filter out bad data\n",
    "    df_info = df_info[df_info['Sector'] != 'N/A']\n",
    "    \n",
    "    df_info.to_csv(INFO_OUTPUT_FILE, index=False)\n",
    "    logging.info(f\"--- Successfully saved company info to {INFO_OUTPUT_FILE} ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(TICKER_DIR, exist_ok=True)\n",
    "    \n",
    "    # --- Load Ticker List ---\n",
    "    try:\n",
    "        df_tickers = pd.read_csv(TICKER_FILE)\n",
    "        tickers_list = df_tickers['Ticker'].tolist()\n",
    "        logging.info(f\"Loaded {len(tickers_list)} unique tickers from {TICKER_FILE}.\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Could not find {TICKER_FILE}. Please run Script 1 first.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading ticker file: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # --- Run Data Fetching Processes ---\n",
    "    get_specific_prices(tickers_list)\n",
    "    fetch_company_info(tickers_list)\n",
    "    \n",
    "    logging.info(\"All data fetching complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed3434f",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb79b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "TICKER_DIR = \"index_market_data\"\n",
    "\n",
    "# --- COLOR CONTRAST TUNING ---\n",
    "\n",
    "# This defines the % change for full red/green. 0.015 = 1.5%\n",
    "COLOR_RANGE_LIMIT = 0.04\n",
    "\n",
    "# --- MODIFICATION: Changed to Red-White-Green ---\n",
    "# This is our custom high-contrast scale.\n",
    "# 0.5 (midpoint) is White.\n",
    "HIGH_CONTRAST_SCALE = [\n",
    "    [0.0, '#CC0000'],   # Dark Red (at -1.5% or lower)\n",
    "    [0.499, '#FF8888'], # Light Red (just before zero)\n",
    "    [0.5, '#FFFFFF'],   # White (exactly at zero)\n",
    "    [0.501, \"#00FF00\"], # Light Green (just after zero)\n",
    "    [1.0, \"#006D00\"]    # Dark Green (at +1.5% or higher)\n",
    "]\n",
    "# --- END COLOR CONTRAST TUNING ---\n",
    "\n",
    "\n",
    "# --- File Paths ---\n",
    "MAP_FILE = os.path.join(TICKER_DIR, \"ticker_to_index_map.csv\")\n",
    "INFO_FILE = os.path.join(TICKER_DIR, \"company_info.csv\")\n",
    "PRICE_FILE = os.path.join(TICKER_DIR, \"ticker_historical_prices.csv\")\n",
    "OUTPUT_HTML_FILE = os.path.join(TICKER_DIR, \"market_treemap.html\")\n",
    "\n",
    "def create_treemap():\n",
    "    logging.info(\"--- Creating Treemap ---\")\n",
    "    \n",
    "    # --- 1. Load all data sources ---\n",
    "    try:\n",
    "        df_map = pd.read_csv(MAP_FILE)\n",
    "        df_info = pd.read_csv(INFO_FILE)\n",
    "        df_prices = pd.read_csv(PRICE_FILE)\n",
    "        logging.info(f\"Loaded {len(df_map)} mappings, {len(df_info)} company infos, {len(df_prices)} prices.\")\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"Error loading file: {e}\")\n",
    "        logging.error(\"Please make sure you have run Script 1 and Script 2 (and let Script 2 finish!)\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Prepare and Merge Data ---\n",
    "    \n",
    "    # Calculate all 3 time-period changes\n",
    "    df_prices['1-Day PctChange'] = (\n",
    "        (df_prices['T-0 (Most Recent)'] - df_prices['T-1 (Previous Day)']) / \n",
    "         df_prices['T-1 (Previous Day)']\n",
    "    )\n",
    "    df_prices['1-Week PctChange'] = (\n",
    "        (df_prices['T-0 (Most Recent)'] - df_prices['T-5 (~1 Week Ago)']) / \n",
    "         df_prices['T-5 (~1 Week Ago)']\n",
    "    )\n",
    "    df_prices['1-Month PctChange'] = (\n",
    "        (df_prices['T-0 (Most Recent)'] - df_prices['T-21 (~1 Month Ago)']) / \n",
    "         df_prices['T-21 (~1 Month Ago)']\n",
    "    )\n",
    "    \n",
    "    df_prices_final = df_prices[['Ticker', '1-Day PctChange', '1-Week PctChange', '1-Month PctChange']]\n",
    "\n",
    "    # Merge the data\n",
    "    logging.info(\"Merging dataframes...\")\n",
    "    df_merged_1 = pd.merge(df_map, df_info, on=\"Ticker\", how=\"inner\")\n",
    "    logging.info(f\"After merging map + info: {len(df_merged_1)} rows remaining.\")\n",
    "    \n",
    "    df_final = pd.merge(df_merged_1, df_prices_final, on=\"Ticker\", how=\"inner\")\n",
    "    logging.info(f\"After merging prices: {len(df_final)} rows remaining.\")\n",
    "\n",
    "    # --- DEBUGGING for your \"Only Nifty50\" problem ---\n",
    "    if len(df_final) < 500: # Check if the final data is suspiciously small\n",
    "        logging.warning(\"--- WARNING ---\")\n",
    "        logging.warning(f\"Final dataset is very small ({len(df_final)} rows).\")\n",
    "        logging.warning(\"This strongly suggests your 'company_info.csv' file is incomplete.\")\n",
    "        logging.warning(\"Please re-run Script 2 and let it finish fetching all tickers.\")\n",
    "        logging.warning(\"--- END WARNING ---\")\n",
    "\n",
    "    # Clean up\n",
    "    df_final.dropna(inplace=True)\n",
    "    df_final = df_final[df_final['MarketCap'] > 0]\n",
    "    \n",
    "    if df_final.empty:\n",
    "        logging.error(\"After merging and cleaning, the final DataFrame is empty. Cannot plot.\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"Final data prepared for plotting with {len(df_final)} valid entries.\")\n",
    "\n",
    "    # --- 3. Plot the Treemap ---\n",
    "    logging.info(\"Generating Plotly treemaps...\")\n",
    "\n",
    "    treemap_path = [px.Constant(\"All Indices\"), 'Index', 'Sector', 'Ticker']\n",
    "    hover_data_formats = {\n",
    "        'MarketCap': ':,.0f',\n",
    "        '1-Day PctChange': ':.2%',\n",
    "        '1-Week PctChange': ':.2%',\n",
    "        '1-Month PctChange': ':.2%'\n",
    "    }\n",
    "    \n",
    "    # We REMOVE the color arguments from here\n",
    "    fig_1d = px.treemap(\n",
    "        df_final, path=treemap_path, values='MarketCap', color='1-Day PctChange',\n",
    "        hover_data=hover_data_formats\n",
    "    )\n",
    "\n",
    "    fig_1w = px.treemap(\n",
    "        df_final, path=treemap_path, values='MarketCap', color='1-Week PctChange',\n",
    "        hover_data=hover_data_formats\n",
    "    )\n",
    "\n",
    "    fig_1m = px.treemap(\n",
    "        df_final, path=treemap_path, values='MarketCap', color='1-Month PctChange',\n",
    "        hover_data=hover_data_formats\n",
    "    )\n",
    "\n",
    "\n",
    "    # --- 4. Combine into a single figure with a dropdown ---\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(fig_1d.data[0])\n",
    "    fig.add_trace(fig_1w.data[0])\n",
    "    fig.add_trace(fig_1m.data[0])\n",
    "\n",
    "    fig.data[1].visible = False\n",
    "    fig.data[2].visible = False\n",
    "\n",
    "    buttons = [\n",
    "        dict(\n",
    "            label=\"1-Day Change\",\n",
    "            method=\"update\",\n",
    "            args=[{\"visible\": [True, False, False]}]\n",
    "        ),\n",
    "        dict(\n",
    "            label=\"1-Week Change\",\n",
    "            method=\"update\",\n",
    "            args=[{\"visible\": [False, True, False]}]\n",
    "        ),\n",
    "        dict(\n",
    "            label=\"1Am-Month Change\",\n",
    "            method=\"update\",\n",
    "            args=[{\"visible\": [False, False, True]}]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # --- 5. Customize and Save ---\n",
    "    \n",
    "    fig.update_layout(\n",
    "        updatemenus=[\n",
    "            dict(\n",
    "                active=0,\n",
    "                buttons=buttons,\n",
    "                direction=\"down\",\n",
    "                pad={\"r\": 10, \"t\": 10},\n",
    "                showactive=True,\n",
    "                x=0.01,\n",
    "                xanchor=\"left\",\n",
    "                y=1.1,\n",
    "                yanchor=\"top\"\n",
    "            )\n",
    "        ],\n",
    "        title='Global Market Treemap by Index and Sector',\n",
    "        margin=dict(t=80, l=25, r=25, b=25),\n",
    "        \n",
    "        # This one coloraxis applies to all traces, guaranteeing\n",
    "        # a stationary scale centered at 0.\n",
    "        coloraxis=dict(\n",
    "            colorscale=HIGH_CONTRAST_SCALE, # Use our custom high-contrast Red-White-Green scale\n",
    "            cmin=-COLOR_RANGE_LIMIT,        # Set the stationary minimum\n",
    "            cmax=COLOR_RANGE_LIMIT,         # Set the stationary maximum\n",
    "            cmid=0,                         # Force 0 to be the center (White)\n",
    "            showscale=True,                 # Show the color bar\n",
    "            colorbar_title='Pct. Change'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Let Plotly's smart-text handle readability\n",
    "    \n",
    "    fig.write_html(OUTPUT_HTML_FILE)\n",
    "    logging.info(f\"--- Successfully saved interactive treemap to: {OUTPUT_HTML_FILE} ---\")\n",
    "    \n",
    "    # Optional: Also display the figure now\n",
    "    # fig.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_treemap()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
